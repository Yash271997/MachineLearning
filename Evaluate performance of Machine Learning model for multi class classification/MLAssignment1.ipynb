{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "Actual Class      0   1   2   3\n",
      "Predicted Class                \n",
      "0                33   1   3   2\n",
      "1                 8  32   2   3\n",
      "2                 4   3  29   3\n",
      "3                 4   0   6  27\n"
     ]
    }
   ],
   "source": [
    "#Reading .csv File\n",
    "import pandas as pd\n",
    "df = pd.read_excel('data.csv')\n",
    "#df\n",
    "#Converting the csv file into Confusion Matrix\n",
    "df_confusion = pd.crosstab(df['Predicted Class'],df['Actual Class'])\n",
    "print(\"Confusion Matrix\")\n",
    "actual = df['Actual Class']\n",
    "predicted = df['Predicted Class']\n",
    "print(df_confusion)\n",
    "#df_confusion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "#Initializing the Weight list for each Class\n",
    "Class_Weight = [0,0,0,0]\n",
    "sum = 0 \n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        sum = sum + df_confusion[i][j]\n",
    "    Class_Weight[i] = sum\n",
    "    sum = 0\n",
    "#Total Weight\n",
    "Total_Weight = Class_Weight[0]+Class_Weight[1]+Class_Weight[2]+Class_Weight[3]\n",
    "#Total_Weight = sum(Class_Weight)\n",
    "print(Total_Weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Accurancy\n",
    "def Calculate_Accuracy():\n",
    "    Class0_Accuracy = (df_confusion[0][0])/(df_confusion[0][1]+df_confusion[0][2]+df_confusion[0][3]+df_confusion[0][0])\n",
    "    Class1_Accuracy = (df_confusion[1][1])/(df_confusion[1][0]+df_confusion[1][1]+df_confusion[1][2]+df_confusion[1][3])\n",
    "    Class2_Accuracy = (df_confusion[2][2])/(df_confusion[2][0]+df_confusion[2][1]+df_confusion[2][2]+df_confusion[2][3])\n",
    "    Class3_Accuracy = (df_confusion[3][3])/(df_confusion[3][0]+df_confusion[3][1]+df_confusion[3][2]+df_confusion[3][3])\n",
    "    #OverAll_Accurancy = (Class0_Accuracy+Class1_Accuracy+Class2_Accuracy+Class3_Accuracy)/4\n",
    "    OverAll_Accurancy = (df_confusion[0][0]+df_confusion[1][1]+df_confusion[2][2]+df_confusion[3][3])/Total_Weight\n",
    "    #Printing all results\n",
    "    print(\"Over All Accurancy and Each Class Accuracy\")\n",
    "    print(\"Class 0 Accurancy = {}\".format(Class0_Accuracy*100),\"%\")\n",
    "    print(\"Class 1 Accurancy = {}\".format(Class1_Accuracy*100),\"%\")\n",
    "    print(\"Class 2 Accurancy = {}\".format(Class2_Accuracy*100),\"%\")\n",
    "    print(\"Class 3 Accurancy = {}\".format(Class3_Accuracy*100),\"%\")\n",
    "    print(\"Overall Accuracy = {}\".format(OverAll_Accurancy*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over All Accurancy and Each Class Accuracy\n",
      "Class 0 Accurancy = 67.3469387755102 %\n",
      "Class 1 Accurancy = 88.88888888888889 %\n",
      "Class 2 Accurancy = 72.5 %\n",
      "Class 3 Accurancy = 77.14285714285715 %\n",
      "Overall Accuracy = 75.625 %\n"
     ]
    }
   ],
   "source": [
    "Calculate_Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Precision, Recall and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Values of Each Class\n",
      "Class 0 Precision = 84.61538461538461 %\n",
      "Class 1 Precision = 71.11111111111111 %\n",
      "Class 2 Precision = 74.35897435897436 %\n",
      "Class 3 Precision = 72.97297297297297 %\n",
      "\n",
      "Macro Average of Precision\n",
      "Macro Average of Precision = 75.76461076461077 %\n",
      "Average Weight for Precision\n",
      "Average Weight for Precision = 76.46604296604296 %\n",
      "\n",
      "Recall Values of Each Class\n",
      "Class 0 Recall = 67.3469387755102 %\n",
      "Class 1 Recall = 88.88888888888889 %\n",
      "Class 2 Recall = 72.5 %\n",
      "Class 3 Recall = 77.14285714285715 %\n",
      "\n",
      "Macro Average of Recall\n",
      "Macro Average of Recall = 76.46967120181407 %\n",
      "Average Weight for Recall\n",
      "Average Weight for Recall = 75.625 %\n",
      "\n",
      "Class 0 F1 Score = 74.99999999999999 %\n",
      "Class 1 F1 Score = 79.01234567901234 %\n",
      "Class 2 F1 Score = 73.41772151898734 %\n",
      "Class 3 F1 Score = 74.99999999999999 %\n"
     ]
    }
   ],
   "source": [
    "######################################### Function to Calculate the Precision #############################################\n",
    "def Calculate_Precision():\n",
    "    #Calculating the precision for each class\n",
    "    Class0_Precision = (df_confusion[0][0])/(df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][0])\n",
    "    Class1_Precision = (df_confusion[1][1])/(df_confusion[0][1]+df_confusion[1][1]+df_confusion[2][1]+df_confusion[3][1])\n",
    "    Class2_Precision = (df_confusion[2][2])/(df_confusion[0][2]+df_confusion[1][2]+df_confusion[2][2]+df_confusion[3][2])\n",
    "    Class3_Precision = (df_confusion[3][3])/(df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[3][3])\n",
    "    \n",
    "    \n",
    "    #Calculating Macro average weight for precision\n",
    "    MacroAverage_Precision = ((Class0_Precision+Class1_Precision+Class2_Precision+Class3_Precision)/4)*100\n",
    "    \n",
    "    #Calculating Average Weight for precision\n",
    "    AverageWeight_Precision = ((Class_Weight[0]*Class0_Precision)+(Class_Weight[1]*Class1_Precision)+(Class_Weight[2]*Class2_Precision)+(Class_Weight[3]*Class3_Precision))/(Total_Weight)\n",
    "    \n",
    "    #Printing the results\n",
    "    print(\"Precision Values of Each Class\")\n",
    "    print(\"Class 0 Precision = {}\".format(Class0_Precision*100),\"%\")\n",
    "    print(\"Class 1 Precision = {}\".format(Class1_Precision*100),\"%\")\n",
    "    print(\"Class 2 Precision = {}\".format(Class2_Precision*100),\"%\")\n",
    "    print(\"Class 3 Precision = {}\".format(Class3_Precision*100),\"%\")\n",
    "    print()\n",
    "    print(\"Macro Average of Precision\")\n",
    "    print(\"Macro Average of Precision =\",MacroAverage_Precision,\"%\")\n",
    "    print(\"Average Weight for Precision\")\n",
    "    print(\"Average Weight for Precision =\", AverageWeight_Precision*100,\"%\")\n",
    "    print()\n",
    "    \n",
    "    return [Class0_Precision,Class1_Precision,Class2_Precision,Class3_Precision]\n",
    "\n",
    "############################################# Function to Calculate the Recall ###########################################\n",
    "def Calculate_Recall():\n",
    "    #Calculating Recall for each class\n",
    "    Class0_Recall = (df_confusion[0][0])/(df_confusion[0][0]+df_confusion[0][1]+df_confusion[0][2]+df_confusion[0][3])\n",
    "    Class1_Recall = (df_confusion[1][1])/(df_confusion[1][0]+df_confusion[1][1]+df_confusion[1][2]+df_confusion[1][3])\n",
    "    Class2_Recall = (df_confusion[2][2])/(df_confusion[2][0]+df_confusion[2][1]+df_confusion[2][2]+df_confusion[2][3])\n",
    "    Class3_Recall = (df_confusion[3][3])/(df_confusion[3][0]+df_confusion[3][1]+df_confusion[3][2]+df_confusion[3][3])\n",
    "    \n",
    "    #Calculating Macro average weight for recall\n",
    "    MacroAverage_Recall = ((Class0_Recall+Class1_Recall+Class2_Recall+Class3_Recall)/4)*100\n",
    "    \n",
    "    #Calculating Average Weight for recall\n",
    "    AverageWeight_Recall = ((Class_Weight[0]*Class0_Recall)+(Class_Weight[1]*Class1_Recall)+(Class_Weight[2]*Class2_Recall)+(Class_Weight[3]*Class3_Recall))/(Total_Weight)\n",
    "    \n",
    "    print(\"Recall Values of Each Class\")\n",
    "    print(\"Class 0 Recall = {}\".format(Class0_Recall*100),\"%\")\n",
    "    print(\"Class 1 Recall = {}\".format(Class1_Recall*100),\"%\")\n",
    "    print(\"Class 2 Recall = {}\".format(Class2_Recall*100),\"%\")\n",
    "    print(\"Class 3 Recall = {}\".format(Class3_Recall*100),\"%\")\n",
    "    print()\n",
    "    print(\"Macro Average of Recall\")\n",
    "    print(\"Macro Average of Recall =\",MacroAverage_Recall,\"%\")\n",
    "    print(\"Average Weight for Recall\")\n",
    "    print(\"Average Weight for Recall =\", AverageWeight_Recall*100,\"%\")\n",
    "    print()\n",
    "    \n",
    "    return [Class0_Recall,Class1_Recall,Class2_Recall,Class3_Recall]\n",
    "\n",
    "######################################## Function to calculate F1 Score ###################################################\n",
    "def Calculate_F1(CP,CR):#,CP1,CP2,CP3,CR0,CR1\n",
    "    #CP = Class Precision\n",
    "    #CR = Class Recall\n",
    "\n",
    "    #calculating F1 for each class\n",
    "    Class0_F1= 2/((1/CP[0])+(1/CR[0]))*100\n",
    "    Class1_F1= 2/((1/CP[1])+(1/CR[1]))*100\n",
    "    Class2_F1= 2/((1/CP[2])+(1/CR[2]))*100\n",
    "    Class3_F1= 2/((1/CP[3])+(1/CR[3]))*100\n",
    "    \n",
    "    #Printing results\n",
    "    print(\"Class 0 F1 Score =\",Class0_F1,\"%\")\n",
    "    print(\"Class 1 F1 Score =\",Class1_F1,\"%\")\n",
    "    print(\"Class 2 F1 Score =\",Class2_F1,\"%\")\n",
    "    print(\"Class 3 F1 Score =\",Class3_F1,\"%\")\n",
    "\n",
    "\n",
    "Precision = Calculate_Precision()\n",
    "Recall = Calculate_Recall()\n",
    "Calculate_F1(Precision,Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Type 1 and Type 2 Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 error for Class 0 = 6.382978723404255 %\n",
      "Type 1 error for Class 1 = 12.745098039215685 %\n",
      "Type 1 error for Class 2 = 9.803921568627452 %\n",
      "Type 1 error for Class 3 = 9.615384615384617 %\n",
      "\n",
      "Type 2 error for Class 0 = 50.0 %\n",
      "Type 2 error for Class 1 = 44.827586206896555 %\n",
      "Type 2 error for Class 2 = 50.0 %\n",
      "Type 2 error for Class 3 = 51.78571428571429 %\n"
     ]
    }
   ],
   "source": [
    "#Calculating Type 1 anf Type 2 Error\n",
    "def Type1AND2Error():\n",
    "    #Calculating Type 1 Error\n",
    "    Class0_Type1 = (df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0])/(df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[1][1]+df_confusion[2][2]+df_confusion[3][3])\n",
    "    Class1_Type1 = (df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1])/(df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][0]+df_confusion[2][2]+df_confusion[3][3])\n",
    "    Class2_Type1 = (df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2])/(df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][0]+df_confusion[1][1]+df_confusion[3][3])\n",
    "    Class3_Type1 = (df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])/(df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[0][0]+df_confusion[1][1]+df_confusion[2][2])\n",
    "    \n",
    "    Class0_Type2 = (df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])/(df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[0][0])\n",
    "    Class1_Type2 = (df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])/(df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[1][1])\n",
    "    Class2_Type2 = (df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])/(df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[2][2])\n",
    "    Class3_Type2 = (df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2])/(df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[3][3])\n",
    "    \n",
    "    #printing Type 1 and 2 Error Values\n",
    "    print(\"Type 1 error for Class 0 =\",Class0_Type1*100,\"%\")\n",
    "    print(\"Type 1 error for Class 1 =\",Class1_Type1*100,\"%\")\n",
    "    print(\"Type 1 error for Class 2 =\",Class2_Type1*100,\"%\")\n",
    "    print(\"Type 1 error for Class 3 =\",Class3_Type1*100,\"%\")\n",
    "    print()\n",
    "    print(\"Type 2 error for Class 0 =\",Class0_Type2*100,\"%\")\n",
    "    print(\"Type 2 error for Class 1 =\",Class1_Type2*100,\"%\")\n",
    "    print(\"Type 2 error for Class 2 =\",Class2_Type2*100,\"%\")\n",
    "    print(\"Type 2 error for Class 3 =\",Class3_Type2*100,\"%\")\n",
    "\n",
    "Type1AND2Error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.f: Calculations Using Sklearn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Overall Accuracy from sklearn library = 75.625 %\n",
      "Precision\n",
      "Macro Precision = 75.76461076461077 %\n",
      "Weighted Precision = 76.46604296604296 %\n",
      "Precision list Class wise = [84.61538462 71.11111111 74.35897436 72.97297297]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "print(\"Accuracy\")\n",
    "print(\"Overall Accuracy from sklearn library =\",(accuracy_score(df['Predicted Class'],df['Actual Class']))*100,\"%\")\n",
    "print(\"Precision\")\n",
    "print(\"Macro Precision =\",(precision_score(actual,predicted,average='macro'))*100,\"%\")\n",
    "print(\"Weighted Precision =\",(precision_score(actual,predicted,average='weighted'))*100,\"%\")\n",
    "print(\"Precision list Class wise =\",(precision_score(actual,predicted,average=None))*100)\n",
    "#Same as above calculation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score\n",
      "Macro recall = 76.46967120181407 %\n",
      "Weight Average recall = 75.625 %\n",
      "Recall list Class wise = [67.34693878 88.88888889 72.5        77.14285714]\n",
      "F1 Score\n",
      "Macro F1Score = 75.60751679949992 %\n",
      "Weight Average F1Score = 75.5072081575246 %\n",
      "F1Score list Class wise = [75.         79.01234568 73.41772152 75.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Recall Score\")\n",
    "print(\"Macro recall =\",(recall_score(actual,predicted,average = 'macro'))*100,\"%\")\n",
    "print(\"Weight Average recall =\",(recall_score(actual,predicted,average='weighted'))*100,\"%\")\n",
    "print(\"Recall list Class wise =\",(recall_score(actual,predicted,average=None))*100)\n",
    "print(\"F1 Score\")\n",
    "print(\"Macro F1Score =\",(f1_score(actual,predicted,average = 'macro'))*100,\"%\")\n",
    "print(\"Weight Average F1Score =\",(f1_score(actual,predicted,average='weighted'))*100,\"%\")\n",
    "print(\"F1Score list Class wise =\",(f1_score(actual,predicted,average=None))*100,)\n",
    "#Same as above calculation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 2.g: Evaluation via Detection Rate and Detection Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Detection Rate =  20.625 %\n",
      "Class 1 Detection Rate =  20.0 %\n",
      "Class 2 Detection Rate =  18.125 %\n",
      "Class 3 Detection Rate =  16.875 %\n",
      "\n",
      "Class 0 Detection Prevalence =  24.375 %\n",
      "Class 1 Detection Prevalence =  26.875 %\n",
      "Class 2 Detection Prevalence =  24.375 %\n",
      "Class 3 Detection Prevalence =  23.125 %\n"
     ]
    }
   ],
   "source": [
    "def CalculateDetectionRate():\n",
    "    #Detection Rate for each Class\n",
    "    Class0_DR = df_confusion[0][0]/Total_Weight\n",
    "    Class1_DR = df_confusion[1][1]/Total_Weight\n",
    "    Class2_DR = df_confusion[2][2]/Total_Weight\n",
    "    Class3_DR = df_confusion[3][3]/Total_Weight\n",
    "    \n",
    "    #Detection Prevalence for each class\n",
    "    Class0_DP = (df_confusion[0][0]+df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0])/Total_Weight\n",
    "    Class1_DP = (df_confusion[0][1]+df_confusion[1][1]+df_confusion[1][2]+df_confusion[1][3])/Total_Weight\n",
    "    Class2_DP = (df_confusion[0][2]+df_confusion[1][2]+df_confusion[2][2]+df_confusion[3][2])/Total_Weight\n",
    "    Class3_DP = (df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3]+df_confusion[3][3])/Total_Weight\n",
    "    \n",
    "    #Printing Values\n",
    "    print(\"Class 0 Detection Rate = \",Class0_DR*100,\"%\")\n",
    "    print(\"Class 1 Detection Rate = \",Class1_DR*100,\"%\")\n",
    "    print(\"Class 2 Detection Rate = \",Class2_DR*100,\"%\")\n",
    "    print(\"Class 3 Detection Rate = \",Class3_DR*100,\"%\")\n",
    "    print()\n",
    "    print(\"Class 0 Detection Prevalence = \",Class0_DP*100,\"%\")\n",
    "    print(\"Class 1 Detection Prevalence = \",Class1_DP*100,\"%\")\n",
    "    print(\"Class 2 Detection Prevalence = \",Class2_DP*100,\"%\")\n",
    "    print(\"Class 3 Detection Prevalence = \",Class3_DP*100,\"%\")\n",
    "    \n",
    "CalculateDetectionRate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.g: Evaluation via Youden's J Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youden's Index for each Class\n",
      "Class 0 J =  0.43617021276595747 %\n",
      "Class 1 J =  0.4242731575388776 %\n",
      "Class 2 J =  0.4019607843137254 %\n",
      "Class 3 J =  0.38598901098901095 %\n"
     ]
    }
   ],
   "source": [
    "def CalulatingYoudenIndex():\n",
    "    #Calculating Youden's J Index\n",
    "    #J = Sensitivity + Specificity - 1\n",
    "    \n",
    "    #Calculating Sensitivity for each class\n",
    "    Class0_Sensitivity = df_confusion[0][0]/(df_confusion[0][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])\n",
    "    Class1_Sensitivity = df_confusion[1][1]/(df_confusion[1][1]+df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])\n",
    "    Class2_Sensitivity = df_confusion[2][2]/(df_confusion[2][2]+df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])\n",
    "    Class3_Sensitivity = df_confusion[3][3]/(df_confusion[3][3]+df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2])\n",
    "    \n",
    "    #Calculating Specificity for each class\n",
    "    Class0_Specificity = (df_confusion[1][1]+df_confusion[2][2]+df_confusion[3][3])/(df_confusion[1][1]+df_confusion[2][2]+df_confusion[3][3]+df_confusion[1][0]+df_confusion[2][0]+df_confusion[3][0])\n",
    "    Class1_Specificity = (df_confusion[0][0]+df_confusion[2][2]+df_confusion[3][3])/(df_confusion[0][0]+df_confusion[2][2]+df_confusion[3][3]+df_confusion[0][1]+df_confusion[2][1]+df_confusion[3][1])\n",
    "    Class2_Specificity = (df_confusion[0][0]+df_confusion[1][1]+df_confusion[3][3])/(df_confusion[0][0]+df_confusion[1][1]+df_confusion[3][3]+df_confusion[0][2]+df_confusion[1][2]+df_confusion[3][2])\n",
    "    Class3_Specificity = (df_confusion[0][0]+df_confusion[1][1]+df_confusion[2][2])/(df_confusion[0][0]+df_confusion[1][1]+df_confusion[2][2]+df_confusion[0][3]+df_confusion[1][3]+df_confusion[2][3])\n",
    "    \n",
    "    #Calculating Younden's Index for each Class\n",
    "    Class0_YoudenIndex = Class0_Sensitivity + Class0_Specificity - 1\n",
    "    Class1_YoudenIndex = Class1_Sensitivity + Class1_Specificity - 1\n",
    "    Class2_YoudenIndex = Class2_Sensitivity + Class2_Specificity - 1\n",
    "    Class3_YoudenIndex = Class3_Sensitivity + Class3_Specificity - 1\n",
    "    \n",
    "    #Printing results\n",
    "    print(\"Youden's Index for each Class\")\n",
    "    print(\"Class 0 J = \",Class0_YoudenIndex,\"%\")\n",
    "    print(\"Class 1 J = \",Class1_YoudenIndex,\"%\")\n",
    "    print(\"Class 2 J = \",Class2_YoudenIndex,\"%\")\n",
    "    print(\"Class 3 J = \",Class3_YoudenIndex,\"%\")\n",
    "    \n",
    "CalulatingYoudenIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
